{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Libraries\n",
      "Reading the data\n",
      "Splitting the dataset\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "\n",
    "#Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import h5py\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Flatten, MaxPool2D, Input, Dropout, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, Callback, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "print('Imported Libraries')\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# #Unzip the dataset\n",
    "# member = ('../GarbageClassification/garbage-classification.zip')\n",
    "# from zipfile import ZipFile\n",
    "# with ZipFile(member, 'r') as zipObj:\n",
    "#        # Extract all the contents of zip file in current directory\n",
    "#     zipObj.extractall()\n",
    " \n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "print('Reading the data')\n",
    "#Reading the data\n",
    "\n",
    "member = ('../Datasets/Garbage classification/Garbage classification/')\n",
    "catagories = os.listdir(member)\n",
    "list_items = []\n",
    "for cat in catagories:\n",
    "    catagory_img = (member  + cat)\n",
    "    #catagory_img.glob('*.jpeg')\n",
    "    for _ in (glob.glob(catagory_img +'/'+'*.jpg')):\n",
    "        list_items.append([cat, _])\n",
    "    \n",
    "#Convert list into dataframe\n",
    "\n",
    "data = pd.DataFrame(list_items,columns = ['catagory', 'filepath'], index = None)\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.head(5)\n",
    "data.shape\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "print('Splitting the dataset')\n",
    "train_data = data[1:2000]\n",
    "val_data = data[2001:2200]\n",
    "test_data = data[2201:2527]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the images\n",
    "\n",
    "#Resizing the images\n",
    "def display_normal(a, title1 = 'Original'):\n",
    "    plt.imshow(a), plt.title(title1)\n",
    "    plt.show()\n",
    "    \n",
    "def display(a,b, title1 = 'Original', title2 = 'Edited'):\n",
    "    plt.subplot(121), plt.imshow(a), plt.title(title1)\n",
    "    plt.subplot(122), plt.imshow(a), plt.title(title2)\n",
    "    plt.show()\n",
    "\n",
    "def adv_preprocessing(image):\n",
    "    #loading images\n",
    "    #Getting 3 images to work with\n",
    "\n",
    "    preimgs = []\n",
    "    img = cv2.imread(image, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    #Setting dimensions to resize\n",
    "    height = 224\n",
    "    width = 224\n",
    "    \n",
    "    dim = (width, height)\n",
    "    res = cv2.resize(img, dim, interpolation = cv2.INTER_LINEAR)\n",
    "    preimgs.append(res)\n",
    "        \n",
    "#Removing noise from image - Gaussian blur\n",
    "    \n",
    "    blurred_img = cv2.GaussianBlur(res, (5,5),0)\n",
    "    preimgs.append(blurred_img)\n",
    "\n",
    "    #Segmentation \n",
    "    #------------------------------------------------------------------\n",
    "    image = res\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    ret,thresh = cv2.threshold(gray, 0,255,cv2.THRESH_BINARY+ cv2.THRESH_OTSU)\n",
    "    \n",
    "    #More noise removal\n",
    "    #------------------------------------------------------------------\n",
    "    kernal = np.ones((3,3), np.uint8)\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernal, iterations=2)\n",
    "    \n",
    "    #Sure background area\n",
    "    sure_bg = cv2.dilate(opening, kernal, iterations = 3)\n",
    "    \n",
    "    #Finding foreground area\n",
    "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "    ret, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
    "    \n",
    "    # Finding unknown region\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "    \n",
    "    #Seperating different objects with different backgrounds\n",
    "    #Markers labelling\n",
    "    ret, markers  = cv2.connectedComponents(sure_fg)\n",
    "    #Add one to all labels so that sure background is 0 not 1\n",
    "    markers = markers+1\n",
    "    \n",
    "    #Mark the unknown region with 0\n",
    "    markers[unknown == 255] = 0\n",
    "    \n",
    "    markers = cv2.watershed(res, markers)\n",
    "    res[markers == -1] = [255,0,0]\n",
    "    placeholder = np.random.rand(224,224)\n",
    "    #Displaying the markers on image\n",
    "    markers = np.dstack([markers,np.zeros((224,224)), placeholder])\n",
    "    #Adding \n",
    "    preimgs.append(res)\n",
    "    preimgs.append(markers)\n",
    "    \n",
    "    return preimgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_gendata(data, batch_size):\n",
    "    labelenc = LabelEncoder()\n",
    "#     dict = {'plastic' : 1, 'glass' : 2, 'cardboard': 3, 'metal': 4, 'paper' : 5,'trash': 6}\n",
    "    n = len(data)\n",
    "    steps = n//batch_size\n",
    "    data['labels'] = labelenc.fit_transform(data['catagory'])\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc_df = pd.DataFrame(enc.fit_transform(data[['labels']]).toarray())\n",
    "\n",
    "    #Defining numpy array to contain label and image data\n",
    "    \n",
    "    batch_data = np.zeros((batch_size, 224,224,3), dtype = np.float32)\n",
    "    batch_labels = np.zeros((batch_size, 6))\n",
    "    indices = np.arange(n)\n",
    "\n",
    "    c1 = 0\n",
    "    i = 0\n",
    "    #Initialize counter\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        next_batch = indices[(i*batch_size):(i+1)*batch_size]\n",
    "        count = 0\n",
    "        \n",
    "\n",
    "        for j, idx in enumerate(next_batch):\n",
    "            if count <= batch_size-4:\n",
    "                img_name = data.iloc[idx]['filepath']\n",
    "                aug_img = adv_preprocessing(data['filepath'].iloc[idx])\n",
    "                encoded_label = enc_df.iloc[idx]\n",
    "                batch_data[count+0] = aug_img[0]\n",
    "                batch_labels[count+0] = encoded_label\n",
    "                batch_data[count+1] = aug_img[1]\n",
    "                batch_labels[count+1] = encoded_label\n",
    "                batch_data[count+2] = aug_img[2]\n",
    "                batch_labels[count+2] = encoded_label\n",
    "                batch_data[count+3] = aug_img[3]\n",
    "                batch_labels[count+3] = encoded_label\n",
    "                count +=4\n",
    "                c1 = c1+1\n",
    "                \n",
    "            else:\n",
    "                count+=1\n",
    "\n",
    "            if count==batch_size:\n",
    "                i += 1\n",
    "                break\n",
    "\n",
    "        i+=1\n",
    "        yield batch_data, batch_labels\n",
    "    \n",
    "    if i>=steps:\n",
    "        i=0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n",
      "8\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_data_gen = adv_gendata(train_data,20)\n",
    "batch_data, batch_labels = next(train_data_gen)\n",
    "\n",
    "print(batch_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0311 22:26:57.710695  4344 deprecation_wrapper.py:119] From c:\\programdata\\miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0311 22:26:57.742697  4344 deprecation_wrapper.py:119] From c:\\programdata\\miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0311 22:26:57.754695  4344 deprecation_wrapper.py:119] From c:\\programdata\\miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0311 22:26:57.778776  4344 deprecation_wrapper.py:119] From c:\\programdata\\miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0311 22:26:57.794695  4344 deprecation_wrapper.py:119] From c:\\programdata\\miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0311 22:26:57.794695  4344 deprecation_wrapper.py:119] From c:\\programdata\\miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0311 22:27:05.615682  4344 deprecation_wrapper.py:119] From c:\\programdata\\miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0311 22:27:05.904941  4344 deprecation.py:506] From c:\\programdata\\miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0311 22:27:05.908937  4344 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Image (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "ConvLayer1 (Conv2D)          (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "Maxpool1 (MaxPooling2D)      (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "ConvLayer3 (Conv2D)          (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "Maxpoo12 (MaxPooling2D)      (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 401408)            0         \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 128)               51380352  \n",
      "_________________________________________________________________\n",
      "FC2 (Dense)                  (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Fc3 (Dense)                  (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 51,474,054\n",
      "Trainable params: 51,473,670\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"In..., outputs=Tensor(\"Fc...)`\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    input_size  = Input(shape = (224,224,3), name  =  'Input_Image')\n",
    "\n",
    "    #Layer 1 - Deapth Layer 1,2\n",
    "    x = Conv2D(64,(3,3), activation = 'relu', padding = 'same', name = 'ConvLayer1' )(input_size)\n",
    "#     x = Conv2D(64,(3,3), activation = 'relu', padding = 'same', name = 'ConvLayer2' )(x)\n",
    "    x = MaxPool2D((2,2), name = 'Maxpool1')(x)\n",
    "    x = BatchNormalization(name = 'bn1')(x)\n",
    "\n",
    "    #Layer 2 - Deapth layer 3,4\n",
    "    x = Conv2D(128,(3,3), activation = 'relu', padding = 'same', name = 'ConvLayer3')(x)\n",
    "#     x = Conv2D(128,(3,3), activation = 'relu', padding = 'same', name = 'ConvLayer4')(x)\n",
    "    x = MaxPool2D((2,2), name = 'Maxpoo12')(x)\n",
    "    x = BatchNormalization(name = 'bn2')(x)\n",
    "#     #Layer 3 - Deapth layer 3\n",
    "#     x = Conv2D(30,(3,3), activation= 'relu',padding = 'same',  name = 'ConvLayer3')(x)\n",
    "#     x = MaxPool2D((2,2), name = 'Maxpool3')(x)\n",
    "\n",
    "    #Flatten the model\n",
    "\n",
    "    x = Flatten(name = 'Flatten')(x)\n",
    "    x = Dense(128, activation = 'relu', name = 'FC1')(x)\n",
    "    x = Dense(128, activation = 'relu', name = 'FC2')(x)\n",
    "    x = Dropout(0.7, name = 'Dropout2')(x)\n",
    "    x = Dense(6, activation = 'softmax', name = 'Fc3')(x)\n",
    "    \n",
    "    model = Model(input = input_size , output = x)\n",
    "    return model\n",
    "\n",
    "#Building the model and summary\n",
    "\n",
    "model = build_model() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0311 22:27:05.999522  4344 deprecation_wrapper.py:119] From c:\\programdata\\miniconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0311 22:27:06.019493  4344 deprecation.py:323] From c:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr = 0.0001, decay = 1e-5)\n",
    "es = EarlyStopping(patience=5)\n",
    "chkpt = ModelCheckpoint(filepath= 'bestmodel',save_best_only=True, save_weights_only=True)\n",
    "model.compile(loss= 'binary_crossentropy', metrics= ['accuracy'], optimizer= opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "20/20 [==============================] - 25s 1s/step - loss: 3.0960 - acc: 0.7417\n",
      "Epoch 2/4\n",
      "20/20 [==============================] - 13s 673ms/step - loss: 3.0461 - acc: 0.7396\n",
      "Epoch 3/4\n",
      "20/20 [==============================] - 13s 638ms/step - loss: 3.1350 - acc: 0.7383\n",
      "Epoch 4/4\n",
      "20/20 [==============================] - 12s 620ms/step - loss: 3.4418 - acc: 0.7283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d16e4e828>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 20\n",
    "epochs = 4\n",
    "model.fit_generator(train_data_gen,epochs=epochs, verbose = 1, steps_per_epoch=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
